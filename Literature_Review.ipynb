{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AMorQ/Literature_Review/blob/main/Literature_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8EJSs6XciY-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def keywords(df):\n",
        "    keyword = df.str.split(';')\n",
        "    ser = pd.Series(np.hstack([keyword[i] for i in range(150)][:150]))\n",
        "    val = pd.value_counts(ser)\n",
        "    claves = val[:25]\n",
        "    return claves\n",
        "\n",
        "def keywords_nlp(df):\n",
        "    keyword = df.str.split(';')\n",
        "    ser = pd.Series(np.hstack([keyword[i] for i in range(150)][:150]))"
      ],
      "metadata": {
        "id": "cIEMIte7dA7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "df_synthese = pd.read_csv('./scopus_synthese.csv', sep=',')\n",
        "df_studies = pd.read_csv('./scopus_studies.csv', sep=',')\n",
        "df_science = pd.read_csv('./scopus_science.csv', sep=',')\n",
        "df_british = pd.read_csv('./scopus_british.csv', sep=',')"
      ],
      "metadata": {
        "id": "ANbA7OVRdJ_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources (run only once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuvCDW7okrnH",
        "outputId": "47c5be67-9d12-4e49-f7cb-5f5d58b3ad09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    # Filter out stopwords and punctuation\n",
        "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
        "    filtered_words = [word for word in words if (word not in stop_words) and (len(word)>2)]\n",
        "\n",
        "    return filtered_words\n",
        "\n",
        "# Function to extract keywords\n",
        "def extract_keywords(abstracts):\n",
        "    keywords = []\n",
        "    for abstract in abstracts:\n",
        "        # Preprocess each abstract\n",
        "        words = preprocess_text(abstract)\n",
        "\n",
        "        # Calculate frequency distribution of words\n",
        "        freq_dist = FreqDist(words)\n",
        "\n",
        "        # Get the most common words (adjust the number as needed)\n",
        "        top_words = freq_dist.most_common(5)  # Change n to the desired number of keywords\n",
        "        keywords.extend([word[0] for word in top_words])\n",
        "\n",
        "    return keywords"
      ],
      "metadata": {
        "id": "XVFBbUI_kwd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l_abstracts = np.concatenate([\n",
        "    df_synthese['Abstract'],\n",
        "    df_studies['Abstract'],\n",
        "    df_science['Abstract'],\n",
        "    df_british['Abstract']\n",
        "    ])"
      ],
      "metadata": {
        "id": "7thlkllVlQ8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_keywords = extract_keywords(l_abstracts)"
      ],
      "metadata": {
        "id": "0AwEQFB4lPph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_keywords[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8Nu16-zlsL_",
        "outputId": "5be1b7c7-9285-4b33-c388-bbbf77a55b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['predictive',\n",
              " 'associated',\n",
              " 'models',\n",
              " 'enactivist',\n",
              " 'cognition',\n",
              " 'social',\n",
              " 'revisionary',\n",
              " 'analysis',\n",
              " 'two',\n",
              " 'may',\n",
              " 'inferences',\n",
              " 'argue',\n",
              " 'generalizations',\n",
              " 'violate',\n",
              " 'norm',\n",
              " 'pain',\n",
              " 'aim',\n",
              " 'paper',\n",
              " 'enactive',\n",
              " 'approach',\n",
              " 'models',\n",
              " 'idealized',\n",
              " 'paper',\n",
              " 'first',\n",
              " 'argue',\n",
              " 'responsibility',\n",
              " 'innovation',\n",
              " 'innovators',\n",
              " 'responsibilities',\n",
              " 'responsible',\n",
              " 'conceptual',\n",
              " 'successful',\n",
              " 'analysis',\n",
              " 'engineering',\n",
              " 'revisions',\n",
              " 'problem',\n",
              " 'predictive',\n",
              " 'dark',\n",
              " 'room',\n",
              " 'one',\n",
              " 'ecological',\n",
              " 'approach',\n",
              " 'coordination',\n",
              " 'organism',\n",
              " 'psychology',\n",
              " 'scientific',\n",
              " 'climate',\n",
              " 'coherence',\n",
              " 'body',\n",
              " 'knowledge']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}